---
title: "Data Analysis in Social Science 3"
author: "Alexey Bessudnov & Max Shilling"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output:
    bookdown::gitbook:
        includes:
          in_header: [docs/google_analytics.html]
        config:
            toc:
                collapse: section
                scroll_highlight: yes
                before: null
                after: null
            toolbar:
                position: fixed
            edit : null
            download: null
            search: yes
            fontsettings:
                theme: white
                family: sans
                size: 2
            sharing: null
documentclass: book
link-citations: yes
github-repo: abessudnov/dataanalysis3
description: "A website for the Data Analysis in Social Science 3 module."
url: 'http\://abessudnov.net/dataanalysis3/'
---
```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```

```{r global_options, include=FALSE}
rm(list=ls()) ### To clear namespace
library(knitr)
opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Introduction

This is a website for the Data Analysis in Social Science 3 module at the University of Exeter (SOC2094/3094 POL2094/3094) as offered in Term 2 in 2018.

The website will be updated with new R Markdown scripts as the course progresses. All the scripts are provided on the 'as is' basis. 

The idea for this module is to teach you how to work with complex longitudinal data sets using data from the [Understanding Society](https://www.understandingsociety.ac.uk/), a longitudinal household survey conducted in the UK. You will learn how to read complex data into R, manipulate and summarise data using dplyr, merge and restructure data frames, visualise data using ggplot2, create statistical reports with R Markdown and (possibly) interactive applications with Shiny. For more details see the [module outline](http://abessudnov.net/dataanalysis3/module-outline.html).

The module is organised according to the "flipped classroom" pedagogy. This means that you read new material and do exercises **before** the class, and in class we work together on the exercises and discuss how what you learned at home can be applied to the Understanding Society data. The readings and exercises mostly come from the [R for Data Science](http://r4ds.had.co.nz/) book by G.Grolemund and H.Wickham.

The pre-requisites for this module are SOC/POL1041 and SOC/POL2077.

All the scripts and other materials are available in the Github repository for the module: https://github.com/abessudnov/dataanalysis3

## Things to do before the next class

Before coming to class 2 please do the following:

1. Register an account with the [UK Data Service](https://www.ukdataservice.ac.uk/), create a data usage (or join the existing data usage that I have created) and download the Understanding Society data in the tab delimited format ([SN6614](https://discover.ukdataservice.ac.uk/catalogue/?sn=6614)). Read the User Manual and familiarise yourself with the structure of the data.

2. Install [Git](https://git-scm.com/) and register an account on [Github](https://github.com/) (if you do not have it already). Then you can either create a new repository for this module or fork my repository (see the link above). Create a project in R Studio for this repository.

To learn how Git and Github work take this free online course: https://www.datacamp.com/courses/introduction-to-git-for-data-science/

3. In the root folder for your project create a folder "data" and put there the Understanding Society data as you downloaded them. The next subfolder must be "UKDA-6614-tab". Never change anything in this folder. Also create an empty "myData" folder in the root folder.

You do not want to track these two folders on Github. To avoid this, include the following two lines in your .gitignore file:

data/

myData/

4. Read ch.11 (Data Import) from R for Data Science and do the exercises - http://r4ds.had.co.nz/data-import.html 



<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Module outline

## Practical arrangements

**Classes:**

* Monday, 11.30am - 1.30pm, WSL 220

Classes begin on 15 January and end on 26 March. The class on 22 January (week 2) has been cancelled and moved to Friday 2 February, 10.30 - 12.30, WSL 220. 


**Office hours (Amory A341)**

* Monday, 2-3pm
* Friday, 12-1pm

**Email:**

* A.Bessudnov [at] exeter.ac.uk


## Aims of the module

This is a fourth module in the data analysis in the social sciences series. In the Introduction to Social Data you learned the basics of descriptive statistics and R. Data Analysis 1 introduced you to statistical inference. Data Analysis 2 covered linear regression analysis. In Data Analysis 3 we are not going to learn new statistical techniques, but will focus on how to apply the techniques you already know to the analysis of real-life data sets and how to produce statistical reports.

This is a skill that you may need in a variety of jobs where data analytic expertise is required, such as marketing analysis, policy analysis in various fields, web analytics, data journalism, academic research, etc.

You already know how to use R to describe data and run simple statistical models. However, real-life data rarely come in the form of a perfectly formatted csv file ready for the analysis. The real life data sets often need to be reshaped, merged, recoded, aggregated and modified in various ways before you can even start your analysis. Unless you know how to do this you will not be able to produce good statistical reports.

This year in this module we will use data from the Understanding Society, a large household panel study conducted in the UK. In the Immigration module we already used the cross-sectional Understanding Society data. In this module we will work with the longitudinal data, which introduces a number of technical challenges.

Throughout the module we will use R for statistical analysis. You are expected to know the basics of data analysis in R.

The only way to learn data analysis is to do data analysis. I will not be able to teach you this, but I can guide your independent learning. This year we will try the "flipped classroom" model of teaching. This means that you will be expected to read and master the required material BEFORE the class and we will use the time in class to answer additional questions and check your solutions rather than introduce new material.

The pre-requisites for this module are POL/SOC1041 and POL/SOC2077.

## Attendance

This module is quite technical. As with other technical skills, missing some initial bits means that you may not be able to catch up. Attendance in this module is crucial. If you do not attend you will not be able to do well in this module. Even skipping a couple of classes will have negative consequences for your understanding of the material. Another negative consequence will be that you will slow the rest of the class down as I will have to explain the same things several times. If you plan not to attend classes please do not take this optional module.

## Assessment

The assessment for this module is a report of 3,500 words (in addition to figures and tables) with the results of statistical analysis you will undertake. This will be 100\% of your final mark for this module. You will be given questions for the reports later in the module. In your analysis you will use the Understanding Society data.

The deadline for submitting your reports through eBart is 29 March at 2pm. You will receive your marks and feedback by 5 May.

Late submissions up to two weeks after the deadline will be capped at 40\%. Submissions that are late for more than two weeks will not be accepted.


## Syllabus plan

I may change some topics as we proceed.


* Data structures in R
* Manipulating data with dplyr
* Longitudinal data in R. Wide and long formats. Reshaping
* Data visualisation with ggplot2
* Producing statistical reports with R Markdown
* Interactive applications with Shiny
* Loops and other control structures. The apply family of functions
* Writing functions in R

## Reading list

The main text for this module:

* G.Grolemund & H.Wickham. (2016). R for Data Science. Freely available at http://r4ds.had.co.nz/

In addition to this you can the following sources (among many others books on R).

* H.Wickham. (2015). ggplot2. Elegant Graphics for Data Analysis. 2nd ed. Springer.
* W.Chang. (2013). R Graphics Cookbook. O'Reilly.
* P.Spector. (2008). Data Manipulation with R. Springer.
* N.Matloff. (2011). The Art of R Programming. No Starch Press.
* H.Wickham. (2014). Advanced R. Chapman & Hall.

<!--chapter:end:moduleOutline.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Assignment

Your assignment for this module is to choose a topic, conduct an independent statistical analysis with the Understanding Society data and write up your results in a report that is about 3,500 words long.

I wanted to give you as much flexibility as possible in preparing the report. I will not provide a detailed guidance on what you should analyse and how you should this. As this is an advanced module I expect you to be able to formulate a research question, identify the data you need and conduct the analysis independently. In other words, the idea is to throw you into the sea of data and find out if you can come up with a nice statistical report that answers a well defined question.

However, there are a few rules.

1. You must use the Understanding Society data and I suggest you use data from the *indresp* files (these are individual adult questionnaires).

2. You must use longitudinal data, i.e. data from more than one wave and preferrably all seven waves of the Understanding Society. This will depend on your question though. If you only have data at two time points available and produce good analysis this is totally fine. But if you have data in all seven waves and only use two this is not fine.

3. Generally you would select one time-varying variable and explore changes over time, in the whole sample and different population subgroups. For example, you may explore how people's incomes changed from 2008 to 2016. You may want to conduct your analysis by gender, age group, location, etc. Other possible variables:
    + Health
    + Employment and job mobility
    + State benefits
    + Any other topic that you find interesting and that has longitudinal data available in the Understanding Society.
 
4. You cannot use data on political interest and political preferences as we will explore these data in class.

5. You must prepare your report in R Markdown. Submit the pdf file (with all the R syntax visible) through eBart. You do no need to submit your work on Github.

5. Please keep your R syntax clear and provide commentary explaining to me what you do.

6. The deadline for your reports is 3 May at 2pm.

I suggest the following steps for your reports.

1. First you need to find a topic that interests you and that has longitudinal data available. Check the User Manual, the questionnaires and data dictionaries for individual waves. Note that the Understanding Society has some modules that are present in each wave and some rotating modules that are only present in one or several waves. Once you have found a variable that interests you make sure that it is present in the data at least at two or more time points.

2. Next you need to read the data into R. Start simple and only read in the data for your outcome variable and maybe sex and age. You will be able to add more variables later as long as you keep your syntax. Clean the data and look at the distributions.

3. Think about the best way to describe and visualise your data. Do you see any interesting patterns and trends? At this stage you should start thinking about the story you want to tell us with your analysis.

4. I do not expect you to do anything fancy statistically. Just providing descriptive statistics and visualisations is fine, as long as I can see that you have thought carefully about which statistics and graphs are best to answer your questions. Every table and chart you have in your report must contribute something to your story. That said, if you want to use some statistical modelling in your report (for example, linear regression) and you do this correctly this will be appreciated and I will give you extra marks for doing this.

5. Once you have a feeling about the general direction of your analysis start adding the details. Maybe you want to explore some more variables; then you need to add them to the data set. For example, for the analysis of political interest I would start with looking at descriptive statistics for political interest for each wave and establish if it was stable or increased/decreased. Then I would think about how I can visualise the results. Then I may start adding details. Was the trend the same for men and women? Different age groups? Different parts of the country? Then if I want to do something fancy I would remember that in 2010 the UK had a general election. Can we get the date of the interview from the data and explore how political interest changed monthly in 2010? And then if I really want to do something very fancy I would read about applyng models with fixed effects to longitudinal data, talk to Alexey in his office hours and explore if change in people's income over time is associated with the change in political interest. (The latter part is optional.)

6. Write up the results.
    + Start with a brief introduction. For political interest that would be approximately the following: Why are we interested in political interest? What happened in British politics between 2008 and 2016 that could affect the level of political interest? Maybe you can find and cite two or three papers that have already explored this topic.
    + Then present your research questions. What are you aiming to achieve with your report? What questions will you answer?
    + Briefly describe the data (variables you are going to use, what waves they are coming from, etc.).
    + Present your statistical results. The structure of this part will depend on your results. This should not be just a collection of tables and graphs. Explain what you see in all those tables and graphs and why you have included them.
    + Discussion. This is a very important part. You need to discuss here how the statistical results you have got contribute to our understanding of your topic (for example, political interest in Britain). Explain in substantive terms your results and discuss them. Why has political interest increased (or decresed, or ramained stable)? What factors contributed to this?
    
7. The length of the report is 3,500 words, but I am not going to count your words and writing slightly more or slightly less is fine. Do not submit 100 pages. In the same way, if your report is obviously too short this is going to affect your mark.

<!--chapter:end:assignment.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Read data {#readdata}

> Prerequisite: Chapter 11 ‘Data Import’ from R for Data Science, available at http://r4ds.had.co.nz/data-import.html

## Introduction to Reading Data

Once you have downloaded the data from the Understanding Society survey, the first thing you need to do is read the data into R. There are a number of ways to do this, and this class will cover loading data into R by using base R and both the readr and data.table packages.

The data we will be loading into R is the individual adult questionnaire answers from wave 1 (*UKDA-6614-tab/tab/us_w1/a_indresp.tab*). This should be saved in the *data* folder you created in your project folder (this was covered in the Introduction section to the course).

## Reading common data files into R

### Ways to read data into R

#### Using base R

To read this data into R using functions from base R, we can first use the **read.table()** function. To read the data in correctly, we need to use **header = TRUE** as the first row of the data contains the names of the variables, and also **stringsAsFactors = FALSE**, which stops R from treating text variables as factors. We can convert these into factors later, when necessary.

```{r cache = TRUE, warning = FALSE, message = FALSE}
UndSoc.1 <- read.table("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab",
                      header = TRUE,
                      stringsAsFactors = FALSE)
```

Different types of files need to read into R in different ways. The **read.table()** function reads data that is *tab separated* (the file will be called 'name*.tab*'), the **read.csv()** function reads data that is *comma separated* (the file will be called 'name*.csv*') and **read.delim()** reads data that is separated in any way. These work in the same way, so once you've mastered one, you've mastered them all.

#### Using the readr package

We can also read data into R using the package *readr*, part of the *tidyverse* package, with the **read_tsv()** command. This is the equivalent command to **read.table()** from base R.

```{r cache = TRUE, warning = FALSE, message = FALSE}
# We can either load the entire 'tidyverse' package into R, which includes the readr package as well as others such as ggplot2, or just the 'readr' package on its own.

library(readr)

UndSoc.2 <- read_tsv("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab")
```

The readr package also allows us to read different types of files into R. The commands **read_csv()** and **read_delim()** read comma and any type of separated data into R respectively.

#### Using the data.table package

The final way we are going to read this data into R is by using the **fread()** function from the *data.table* package. The advantage of the **fread()** command is that it can read any type of separated data into R, without you having to change anything if the file type you are trying to read changes.

```{r cache = TRUE, warning = FALSE, message = FALSE}
library(data.table)

UndSoc.3 <- fread("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab")
```

### Comparing these three methods

One of the important differences between these three methods for reading data into R is the length of time they take to read the data. We can compare this by reading the data into R again, this time wrapping our code with the command **system.time()**.

#### Using base R

```{r cache = TRUE, warning = FALSE, message = FALSE}
# Base R
system.time(UndSoc.1 <- read.table("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab",
                      header = TRUE,
                      stringsAsFactors = FALSE))
```

It takes about 23 seconds to read the data into R using this method.

#### Using the readr package

```{r cache = TRUE, warning = FALSE, message = FALSE}
system.time(UndSoc.2 <- read_tsv("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab"))
```

It takes only 5 seconds to read the data into R using this method.

#### Using the data.table package

```{r cache = TRUE, warning = FALSE, message = FALSE}
system.time(UndSoc.3 <- fread("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab"))
```

It takes just 3 seconds to read the data into R using this method, a whole 20 seconds quicker than using the commands from base R!

From this we can see clear differences in loading time. With small data sets, the difference between these three methods will not be very noticeable or important, but with larger data the increase in loading times the *readr* and *data.table* packages provide can be quite substantial.

### Editing a dataset while reading it into R

#### Skipping rows of data

Sometimes we do not want to read in the entire dataset we have into R, but instead want to ignore the first few rows. For example, if we were to do some sentiment analysis on Hamlet (found here: http://www.gutenberg.org/cache/epub/2265/pg2265.html), we would want to only read in the play itself, not the introductions before the start of the play. Once we've saved the play as a *.txt* file in our data folder, we can use the **skip = ""** command to only read the file from the start of the play, with each line of the script counting as a row.

```{r cache = TRUE, warning = FALSE, message = FALSE}
Hamlet <- read.delim("data/Hamlet.txt",
                     skip = 331,
                     header = FALSE)
```

#### Reading only select variables

When working with large datasets, like the one you will be using for your assignment, there are only certain variables that are of interest. As a result, it makes sense to load in only these variables, to make analysing your data easier to do.

Let's say that we're interested in how political interest is distributed across the UK, and want to analyse how this is differentiated by sex and age. The variables we need to read into R are these two (*a_sex* and *a_dvage*), as well as interest in politics (*a_vote6*) and the personal identification variable for each individual (*a_pidp*). We can use the **select = ""** command to do this.

```{r cache = TRUE, warning = FALSE, message = FALSE}
UndSoc.4 <- fread("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab",
                 select = c("pidp", "a_sex", "a_dvage", "a_vote6"))
```

#### Setting *NA* values

Sometimes, if we look at the raw data we are about to analyse, we see that there are a number of missing values. Instead of recoding these values as *NA* after we have loaded our data (found, in this example, at https://raw.githubusercontent.com/abessudnov/dataanalysis3/master/exData/Table3.txt), we can do this while reading our data into R using the **na.strings = ""** command.

```{r cache = TRUE, warning = FALSE, message = FALSE}
MissingData <- read.table("data/MissingData.txt",
                 header = TRUE,
                 skip = 2,
                 na.strings = c("*", "**", "--"))
```

## Reading other data files

### Excel

In R, you can also read data that was created in Excel. Excel files are saved primarily as *.xls* or *.xlsx* files. Fortunately, after installing the *readxl* package, the command **read_excel()** can read both formats into R. Using the example Excel document found at (https://github.com/abessudnov/dataanalysis3/blob/master/exData/tableExcel.xlsx), we can easily read this into R.

```{r cache = TRUE, warning = FALSE, message = FALSE}
library(readxl)

Excel <- read_excel("data/tableExcel.xlsx")
```

When reading Excel files into R, **read_excel()** defaults to loading the first sheet. If there are multiple sheets in our document (by opening the Excel file we can see that there are 2 in this case), we can load different sheets in with the **sheet = ""** command.

```{r cache = TRUE, warning = FALSE, message = FALSE}
Excel2 <- read_excel("data/tableExcel.xlsx",
                     sheet = 2)
```

However, if we look at the sheet we have just loaded, we can see that there are two *NA* values, one of which is coded as *"NA"*. Furthermore, if we compare how these *NA* values appear in our data with the ones from our *MissingData* dataset, we can see that they aren't actually coded as missing values, but instead as values called *NA* and *"NA"*.

```{r cache = TRUE, warning = FALSE, message = FALSE}
head(MissingData)

head(Excel2)
```

To fix this, we need to set both these values as *NA*, which we can do with the **na = ""** command.

```{r cache = TRUE, warning = FALSE, message = FALSE}
Excel3 <- read_excel("data/tableExcel.xlsx",
                     sheet = 2,
                     na = c('NA', '"NA"'))

# Note that as one of the missing values is called "NA", you have to use inverted commas ('') around the entire value for R to change it to an NA value, rather than the usual speech marks (""). Inverted commas and speech marks are interchangable in R, though if you use one to open a command, you cannot use the other to close it.
```

We can now see that this dataset has true *NA* values in it for the missing data.

```{r cache = TRUE, warning = FALSE, message = FALSE}
head(Excel3)
```

### SPSS

As many people use SPSS instead of R to analyse data, there are times when you will have to use data that is intended for use in SPSS. The *haven* package allows us to do this, and as an example we will read a dataset combining data from the *Big Allied and Dangerous* and the *Global Terrorism Database* databases (found at http://vle.exeter.ac.uk/mod/resource/view.php?id=679842).

```{r cache = TRUE, warning = FALSE, message = FALSE}
library(haven)

SPSS <- read_spss("data/BAAD and GTD merged.sav")
```

## Saving datasets

Once you have read your data into R you can save it as a file so you can load it back into R at a later date to continue working on it. Again, there are a number of different ways to do this.

### Saving data as a text file

Having read the first wave of data into R with only the variables we are interested in (see *Reading only select variables*), we can save this data as a text file using base R, without having to install any new packages. All we need to do is create a folder called *myData* in the folder where you have saved your project, and then you can save your dataset to this folder.

```{r cache = TRUE, warning = FALSE, message = FALSE}
write.csv(UndSoc.4, "myData/UndSoc.4.csv")
```

As you had to get specific permission to access the *Understanding Society* dataset, it is important you do not save this data to Github. This applies to every method of saving data from R we will cover here. Add the following line to your *gitignore* file to keep the *myData* folder untracked:

**myData/**

### Saving data as an Excel file

To save our data as an Excel file, we need to first install the *writexl* package, and use the command **write_xlsx()**.

```{r cache = TRUE, warning = FALSE, message = FALSE}
library(writexl)

write_xlsx(UndSoc.4, path = 'myData/UndSoc.4.xlsx', col_names = TRUE)
```

### Saving data as an SPSS file

We can use the *haven* package we used to read files saved for use in SPSS to write data as SPSS files too, using the **write_sav()** command.

```{r cache = TRUE, warning = FALSE, message = FALSE}
write_sav(UndSoc.4, "myData/UndSoc.4.sav")
```

### Loading these datasets back into R

To read any of these datasets back into R, use what you learnt in *Reading common data files into R*.

## Saving the working environment

If you want to save an entire workspace, rather than just individual files, the command **save.image()** does this. This saves not only all loaded data frames but any saved objects: models, plots, functions, etc.

```{r cache = TRUE, warning = FALSE, message = FALSE}
# Saving the workspace.

save.image("myData/Workspace.RData")

# Clearing the current workspace.

rm(list = ls())

# Loading the workspace back into R.

load("myData/Workspace.RData")
```

## Reading in multiple waves from Understanding Society

For your assignment, you are required to analyse at least two waves of data from the Understanding Society dataset. We will cover how to join multiple waves together in a future lesson, but before doing this, let's read in the first two waves of Understanding Society. To do this, we need to use the function **read_tsv()** from the readr package, as the data are tab separated. We will load this by attaching the *tidyverse* package (you will need to install the tidyverse package first if you have not already done so, with the **install.packages()** function), as we will be using other packages within *tidyverse* in our analysis later.

```{r cache = TRUE, warning = FALSE, message = FALSE}
# First, we clear the current workspace.

rm(list = ls())

# This code attaches the tidyverse package and then reads the first two waves of Understanding Society separately into R.

library(tidyverse)

UndSoc1 <- read_tsv("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab")

UndSoc2 <- read_tsv("data/UKDA-6614-tab/tab/us_w2/b_indresp.tab")
```

As we discussed earlier in this class, these files take a lot longer to load into R than other data files you have used in the past. This is because the files themselves are large, and take up a lot of space in the memory. We can see just how much space they take up by using the **object.size()** command.

```{r cache = TRUE, warning = FALSE, message = FALSE}
object.size(UndSoc1)

object.size(UndSoc2)
```

However, this does not give us an answer that we can easily interpret. Therefore, by using the **format()** command, we can specify how we want R to show us this data. In this case, by adding **units = “”** we can tell R to show us how large the files are in specific units.

```{r cache = TRUE, warning = FALSE, message = FALSE}
# By adding units = "auto" R automatically chooses the clearest way to show us the size of the files.

format(object.size(UndSoc1), units = "auto")

format(object.size(UndSoc2), units = "auto")
```

To make the files we are working with smaller, and thus make R carry out our analysis quicker, we can select only the variables we need for our analysis. We are only going to load a few variables here to keep this simple; you can always add more to your analysis. Here, as we are looking at level of interest in politics, they are:

* pidp: this is the id number given to each respondent to identify them in each wave

* a_sex: sex from wave 1

* a_dvage: age from wave 1

* a_vote6: level of interest in politics from wave 1

* b_sex: sex from wave 2

* b_dvage: age from wave 2

* b_vote6: level of interest in politics from wave 2

By using what we covered earlier, we can select only the variables we want to read into R, making the loading times decrease significantly as they are much smaller datasets.

```{r cache = TRUE, warning = FALSE, message = FALSE}
UndSoc1ed <- fread("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab",
                 select = c("pidp", "a_sex", "a_dvage", "a_vote6"))

UndSoc2ed <- fread("data/UKDA-6614-tab/tab/us_w2/b_indresp.tab",
                 select = c("pidp", "b_sex", "b_dvage", "b_vote6"))
```

We can check that these are much smaller objects, and that our code was successful.

```{r cache = TRUE, warning = FALSE, message = FALSE}
format(object.size(UndSoc1ed), units = "auto")

format(object.size(UndSoc2ed), units = "auto")

head(UndSoc1ed)

head(UndSoc2ed)
```

As we no longer have any use for the original data we loaded into R, we can remove them from R to free up the memory they are unnecessarily taking up.

```{r cache = TRUE, warning = FALSE, message = FALSE}
rm(UndSoc1, UndSoc2)
```

We can then save these datasets so we can easily load them into R at a later date to continue working on them. We will use these in future classes. Here, we save the workspace, which includes just waves 1 and 2 of the Understanding Society data, for future use.

```{r cache = TRUE, warning = FALSE, message = FALSE}
save.image("myData/ReadData.RData")
```

<!--chapter:end:readData.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Transform data {#transformdata}

> Please read ch.5 (Data transformation) from R for Data Science - http://r4ds.had.co.nz/transform.html

Once you have imported your data in R you will usually want to clean it, transform it and produce some data summaries. All these tasks can be accomplished with base R. However, it is usually more convenient to use specialised packages for this, such as **dplyr** and **data.table**. In this module we will use **dplyr**.

Let us read the data from wave 1 of the Understanding Society.

```{r}
library(tidyverse)
W1 <- read_tsv("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab")
```

## The pipe operator (%>%)

Let us start with tabulating a variable for political interest.

```{r}
table(W1$a_vote6)
```

Now if we want to convert this table into a table of proportions we need to do the following.

```{r}
prop.table(table(W1$a_vote6))
```

Imagine now that you want to use the **kable()** function from the **knitr** package to print this table. We will need to convert the table into a data frame first and then apply the function kable.

```{r}
library(knitr)
kable(data.frame(prop.table(table(W1$a_vote6))), digits = 2)
```

At this point we have four nested functions and the code becomes difficult to read. With the pipe operator (%>%) you can achieve the same result with the following code.

```{r}
W1$a_vote6 %>%
  table() %>%
  prop.table() %>%
  data.frame() %>%
  kable(digits = 2)
```

The pipe operator passes the results of the execution of a function to the next function. This makes code easier to write, read and understand.

## Select variables

Imagine we want to select some variables from a data frame. Indeed, *W1* is too large for our purposes and we do not need all the variables today. Let us select the variables for sex, age, place of birth and measures of weight and height.

In base R you could do the following:

```{r}
newW1 <- subset(W1, select = c("pidp", "a_sex", "a_dvage", "a_ukborn", "a_hlht", "a_hlhtf",
                               "a_hlhti", "a_hlhtc", "a_hlwt", "a_hlwts", "a_hlwtp", "a_hlwtk"))
head(newW1, 3)
```

With **dplyr** the following code will produce the same result.


```{r}
newW1 <- W1 %>%
  select(pidp:a_dvage, a_ukborn, a_hlht:a_hlwtk)
head(newW1, 3)
```

Note that I combined together the variables that follow each other in the original data frame with :.

## Select cases

Another common task is to select cases based on some conditions. For example, we may want to have a data frame that only indludes women aged 18 to 25.

In base R you can do the following.

```{r}
women <- newW1[newW1$a_sex == 2 & (newW1$a_dvage >= 18 & newW1$a_dvage <=25),]
```

In **dplyr** the code will be the following:

```{r}
newW1 %>%
  filter(a_sex == 2 & (a_dvage >= 18 & a_dvage <=25)) %>%
  head(3)
```

I am not saving the new data frame as an object here and just print the first three rows from the data to demonstrate the result.

Imagine now you want to select only people born in Wales or Northern Ireland and aged over 40.

```{r}
newW1 %>%
  filter((a_ukborn == 3 | a_ukborn == 4) & a_dvage > 40) %>%
  head(3)
```

## Create new variables

Let us create a new dummy variable *scotland* that takes the value of 1 if a person was born in Scotland and 0 otherwise.

I will not show you how to do this in base R (I assume you know this already) and will focus on **dplyr**.

```{r}
newW1 %>%
  mutate(scotland = ifelse(a_ukborn == 2, 1, 0)) %>%
  head(3)
```

Here I use the function **ifelse** that evaluates a condition that *a_ukborn == 2* and returns 1 if this is true and 0 if this is false.

Now let us imagine we want to code a variable for the place of birth converting numeric values into text and coding missing values as NA.

```{r}
newW1 %>%
  mutate(placeBirth = recode(a_ukborn,
                             "1" = "England",
                             "2" = "Scotland",
                             "3" = "Wales",
                             "4" = "Northern Ireland",
                             "5" = "not UK",
                             .default = NA_character_)) %>%
  head(3)
```

*.default = NA_character_* codes all the values that were not specifically matched (all negative values in our case) to missing values.

We can compare the distributions of the original and recoded variables to make sure that everything is correct.

```{r}
newW1 %>%
  mutate(placeBirth = recode(a_ukborn,
                             "1" = "England",
                             "2" = "Scotland",
                             "3" = "Wales",
                             "4" = "Northern Ireland",
                             "5" = "not UK",
                             .default = NA_character_)) %>%
  count(a_ukborn, placeBirth)
```

Now let us have a slightly more complicated case. We may want to code a variable for the body mass index (BMI) which is defined as weight in kilograms divided by the square of height in meters:

$$BMI = \frac{weight_{kg}}{{height_{m}}^2}$$

The problem is that in our data set some people gave their weight in kilograms (*a_hlwtk*) and some in stones and pounds (*a_hlwts* and *a_hlwtp*). Similarly, some people gave their height in centimeters (*a_hlhtc*) and others in feet and inches (*a_hlhtf* and *a_hlhti*). We need to start with converting the measures for everyone to kilograms and centimeters and then we will be able to create a variable for BMI.

These are the formulas for conversion:

1 feet = 30.48cm
1 inch = 2.54cm
 
1 stone = 6.35kg
1 pound = 0.45kg

```{r}
W1mod <- newW1 %>%
  # create a variable for height in cm
  # if height is measured in feet and inches (a_hlht == 1) and is not missing (a_hlhtf > 0),
  # convert it to centimeters
  # otherwise if it is already measured in cm (a_hlht == 2) leave as it is
  # if a_hlht is neither 1 nor 2 code it to missing
  mutate(heightcm = ifelse(a_hlht == 1 & a_hlhtf > 0, 
                           a_hlhtf*30.48 + a_hlhti*2.54,
                           ifelse(a_hlht == 2 & a_hlhtc > 0, 
                                  a_hlhtc, NA))) %>%
  # now same with weight
  mutate(weightkg = ifelse(a_hlwt == 1 & a_hlwts > 0, 
                           a_hlwts*6.35 + a_hlwtp*0.45,
                           ifelse(a_hlwt == 2 & a_hlwtk > 0, 
                                  a_hlwtk, NA))) %>%
  # now create a variable for BMI
  mutate(bmi = weightkg / (heightcm / 100)^2)

```

Let us now look at the distribution of BMI.

```{r}
hist(W1mod$bmi)
```

## Sort data

We can sort data with **arrange**.

We may want to sort the data by BMI.

```{r}
W1mod %>%
  arrange(bmi) %>%
  select(pidp, bmi) %>%
  head(5)
```

We can also sort cases by BMI in the decreasing order, separatey for each sex.

```{r}
W1mod %>%
  arrange(a_sex, desc(bmi)) %>%
  select(pidp, a_sex, bmi) %>%
  head(5)
```

## Summarise data

**dplyr** is also helpful when you want to create a data frame with summary statistics. For example, we may want to calculate mean and median BMI in our sample and the proportion of people with BMI over 30 (considered obese).

```{r}
W1mod %>%
  # create a binary variable for being obese
  mutate(bmiover30 = ifelse(bmi > 30, 1, 0)) %>%
  summarise(
    meanBMI = mean(bmi, na.rm=TRUE), 
    medianBMI = median(bmi, na.rm=TRUE), 
    proportionObese = mean(bmiover30, na.rm=TRUE)
    )
```

We can also produce summary statistics by group. Let us look at the BMI summaries by age group and by sex.

```{r}
W1mod %>%
  # create a variabe for being obese
  mutate(bmiover30 = ifelse(bmi > 30, 1, 0)) %>%
  # create a variable for age groups
  mutate(agegr = ifelse(a_dvage >= 18 & a_dvage <= 35, "18-35",
                        ifelse((a_dvage >= 36 & a_dvage <= 55), "36-55",
                        ifelse(a_dvage >= 56, ">55", NA)))) %>%
  # filter out people with missing age
  filter(!is.na(agegr)) %>%
  # group by sex and age
  group_by(a_sex, agegr) %>%
  # calculate summary statistics
  summarise(
    meanBMI = mean(bmi, na.rm=TRUE), 
    medianBMI = median(bmi, na.rm=TRUE), 
    proportion = mean(bmiover30, na.rm=TRUE)
  )
```

We can see that people aged 18 to 35 have the lowest proportion of obese people both among men and women.

Let us now save **W1mod** for future use.

```{r}
saveRDS(W1mod, "myData/W1mod.rds")
```





















<!--chapter:end:transformData.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Join data {#joining}

> Prerequisite: Chapter 13 'Relational Data' from R for Data Science, available at http://r4ds.had.co.nz/relational-data.html

## Introduction to joining data

When you work with data, it is rare that all the data you need will be confined to just one table. As a result, you will often find that you have to manipulate multiple data tables, or data frames. In order to do this, it helps to be able to join these multiple sources of data into just one table.

This is very relevant for your assignment, as you are required to work with multiple waves of the Understanding Society data set. Without being able to join these waves together, you will struggle to analyse more than one wave at a time.

This class will teach you a number of ways to do this. We will start by joining just two waves together in the four ways introduced in Chapter 13 of R for Data Science into one, easy to work with, table. The next class will focus on joining multiple waves together at once.

## Loading the data

Before we join the waves together, we first need to read the individual waves into R. To do this, we need to use the function **read_tsv()** from the **readr** package, as the data are tab separated. We can load this by attaching the **tidyverse** package (you will need to install the tidyverse package first if you have not already done so, with the **install.packages()** function).

```{r cache = TRUE, warning = FALSE, message = FALSE}
# This code attaches the tidyverse package and then reads the first two waves of Understanding Society separately into R.
library(tidyverse)

UndSoc1 <- read_tsv("data/UKDA-6614-tab/tab/us_w1/a_indresp.tab")
UndSoc2 <- read_tsv("data/UKDA-6614-tab/tab/us_w2/b_indresp.tab")

```

You may have noticed that this took a lot longer to load into R than other data files you have used in the past. This is because the files themselves are large, and take up a lot of space in the memory. We can see just how much space they take up by using the **object.size()** command.

```{r cache = TRUE}
object.size(UndSoc1)
object.size(UndSoc2)

```

However, this does not give us an answer that we can easily interpret. Therefore, by using the **format()** command, we can specify how we want R to show us this data. In this case, by adding **units = ""** we can tell R to show us how large the files are in specific units.

```{r cache = TRUE}
format(object.size(UndSoc1), units = "auto")
format(object.size(UndSoc2), units = "auto")
# By adding units = 'auto' R automatically chooses the clearest way to show us the size of the files.

```

To make the files we are working with smaller, and thus make R carry out our analysis quicker, we can select only the variables we need for our analysis. We are only going to load a few variables here to keep this simple; you can always add more to your analysis. Here, as we are looking at level of interest in politics, they are:

* **pidp**: this is the id number given to each respondent to identify them in each wave

* **a_sex**: sex from wave 1

* **a_dvage**: age from wave 1

* **a_vote6**: level of interest in politics from wave 1

* **b_sex**: sex from wave 2

* **b_dvage**: age from wave 2

* **b_vote6**: level of interest in politics from wave 2

By using what we learnt in the previous class, *Transforming Data*, we can edit both data sets to keep only the variables we need.

```{r cache = TRUE}
UndSoc1ed <- UndSoc1 %>%
        select(pidp, a_sex, a_dvage, a_vote6)
UndSoc2ed <- UndSoc2 %>%
        select(pidp, b_sex, b_dvage, b_vote6)

```

We can check that these are much smaller objects, and that our code was successful.

```{r cache = TRUE}
format(object.size(UndSoc1ed), units = "auto")
format(object.size(UndSoc2ed), units = "auto")
head(UndSoc1ed)
head(UndSoc2ed)

```

As we no longer have any use for the original data we loaded into R, we can remove them from R to free up the memory they are unnecessarily taking up.

```{r cache = TRUE}
rm(UndSoc1, UndSoc2)
```

## Joining waves 1 and 2

Now our data is ready for us to join the data sets. As you already know, there can be several types of joins. Here, we will use each of them to join the two waves of data together, and look at the differences between them. However, first it is important to understand how joining data works.

### The 'key'

When we join two data frames together, we have to choose which variable we want to join them by, known as the 'key'. As we are working with a data set that has an id number (the variable **pidp** in the data set) that is unique to each respondent, it makes sense to join the two waves we are using by this id number, so we can compare responses by each person in each wave of data. To do this, we use the **by = ""** command.

### Inner join

The first join we will use is the inner join. By using **inner_join()** we can join both waves together and keep the observations that are present in both data frames. This means that respondents who were in both waves 1 and 2 will be in our new data frame, but anyone who appeared in only wave 1 *or* wave 2 will be excluded.

```{r cache = TRUE}
inner <- UndSoc1ed %>%
        inner_join(UndSoc2ed, by = "pidp")
```

We can see that this has excluded a number of respondents from both waves 1 and 2 who only appeared in one wave and not the other, as we only have `r nrow(inner)` observations in the our new data frame compared to `r nrow(UndSoc1ed)` in UndSoc1, and `r nrow(UndSoc2ed)` in UndSoc2.

### Left join

However, we may not want to exclude all of the respondents who do not appear in both waves. If, for example, we were working with three waves of data and wanted to compare how the answers given by respondents in wave 1 changed over time, we would want to keep answers from people who were in waves 1 and 3, even if they were not in wave 2. To do this, we can use **left_join()**, which will keep answers from everyone who appeared in wave 1, and exclude answers from everyone who did not.

```{r cache = TRUE}
left <- UndSoc1ed %>%
        left_join(UndSoc2ed, by = "pidp")
```

To check we have done this correctly, we can compare the number of observations in *UndSoc1ed* and *left*, which should be the same. As we can see they each have `r nrow(left)` observations, we can be confident our new data frame has been created successfully. If any respondents appeared in wave 1 but not wave 2, their answers for wave 2 will show as 'NA' values. We can have a quick look at how this new data frame looks with **head()**.

```{r cache = TRUE}
head(left)
```

### Right join

The command **right_join()** is very similar to **left_join()**, except that instead of keeping all the respondents who were in wave 1, this will keep all the respondents who appeared in wave 2 and exclude those who did not. Again, any respondents who appeared in wave 2 but not wave 1 will have their answers for wave 1 show as 'NA' values.

```{r cache = TRUE}
right <- UndSoc1ed %>%
        right_join(UndSoc2ed, by = "pidp")
head(right)
```

### Full join

Usually, we would want all the respondents from both waves to remain in the data set, no matter if they appeared in the other waves or not. If we need to exclude any respondents from our analysis we can do this manually. We can do a full join with the **full_join()** command.

```{r cache = TRUE}
full <- UndSoc1ed %>%
        full_join(UndSoc2ed, by = "pidp")
```

By doing a full join, our new data frame has `r nrow(full)` observations compared to  `r nrow(UndSoc1ed)` in wave 1 and `r nrow(UndSoc2ed)` in wave 2. This is because it includes not only all the individuals that took part in *both* waves 1 and 2, but also those who only took part in either wave 1 *or* wave 2.

Now that we have joined two waves together, we could begin our analysis of these waves, if we had decided to just analyse the first couple of waves from the *Understanding Society* dataset.

```{r cache = TRUE}
head(full)
```

If you wanted to work with more than just these two waves, you could manually join each new wave to our 'full' data frame. However, this is a slow and cumbersome process, so the next class will teach you how to join several waves of data together at once, using *iteration* and *loops*.

<!--chapter:end:joiningData.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Iteration {#iteration}

> Please read ch.21 (Iteration) from R for Data Science - http://r4ds.had.co.nz/iteration.html

In the Understanding Society data we have seven waves and seven separate individual files for adult questionnaires. We will need to read them all for the data to be joined. Of course, we can read them one by one, but this is inconvenient.

We will use this example to learn about iteration, one of the most important concepts in programming. You should read ch.21 from R for Data Science and do the exercises to learn the basics; here we will consider how we can apply iteration to our case.

Let us first consider a very simple **for** loop.

```{r}
for (i in 1:5) {
  print(i)
}
```

This loop goes through the values from 1 to 5 and in each iteration it prints the number on the screen. With the Understanding Society data, we want to go from 1 to 7 (as we have seven waves) and in each iteration we want to read in the data and join it to the data from other waves. Let us see how we can write a loop that does it.

First, we need to identify the files we want to open. The **dir** function will return the paths and names of all the data files in our data folder that contain the pattern *indresp*.

```{r}
files <- dir("data/UKDA-6614-tab/tab",
             pattern="indresp", recursive = TRUE, full.names=TRUE)
files
```

There are 25 files as we also have data from the BHPS, not just the Understanding Society. We do not need the BHPS, so we want to select only the files from the Understanding Society. We can use the function **str_detect** from the package **stringr** to select only the files whose paths contain *us*.

```{r}
# stringr will return a logical vector. Note that I specify which package the function comes from
# without explicitly attaching it.
stringr::str_detect(files, "us")
# Now I only select the files from UndSoc
files <- files[stringr::str_detect(files, "us")]
files
```

Now we have a vector of file names we want to loop over. We can write a short loop that prints the path and files name.

```{r}
for (i in 1:7) {
  print(files[i])
}

```

Note that the same task can be achieved simply with:

```{r}
for (i in files) {
  print(i)
}
```

You will see a bit later why I wanted to loop over numbers rather than elements of the character vector.

Now we need to read in the data. We can read the whole files, but this is inefficient as we will only need a few variables. The function **fread** from the package **data.table** allows us to specify the variables we want to read. Let us choose the id variable (**pidp**), sex, age, interest in politics and net monthly income. The problem is that in each wave these variables have different names indicated by a prefix. **pidp** does not change and has the same name in each wave. All the other variables have a prefix **a_** in wave 1, **b_** in wave 2, etc. We will need to find a way to loop over not just file names in **files**, but also prefixes at the same time.

Let us start with creating a vector of the variable names without the prefixes.

```{r}
vars <- c("sex", "dvage", "vote6", "fimnnet_dv")
```

If we want to add a prefix to the elements of this vector we can use the function **paste**.

```{r}
paste("a", vars, sep = "_")
```

The constant **letters** contains all the letters of the English alphabet, so the same expression can be written as the following.

```{r}
paste(letters[1], vars, sep = "_")
```

Now we can write a loop that goes through the values 1 to 7 and in each iteration reads the correct data file choosing the variables with the correct prefix.

```{r}
# Attach data.table
library(data.table)
for (i in 1:7) {
        # Create a vector of the variables with the correct prefix.
        varsToSelect <- paste(letters[i], vars, sep = "_")
        # Add pidp to this vector (no prefix for pidp)
        varsToSelect <- c("pidp", varsToSelect)
        # Now read the data. 
        data <- fread(files[i], select = varsToSelect)
        # print the first line
        print(head(data, 1))
}        
```

Now we need to join all these data frames together, and we want to do this in the loop. It is clear what we need to do in the second and later iterations of the loop: join the data from wave 2 with the data from wave 1, etc. But what shall we do in the first iteration? There is no data frame yet to be joined with the data from wave 1. Clearly our algorithm for the first iteration needs to be different from the algorithm for all other iterations. We will use the **if ... else** control structure for this.

In the first iteration of the loop we simply want to save the data from wave 1. In the second and other iterations we want the data to be joined with the data frame we have from the previous iteration.

```{r}
for (i in 1:7) {
        # Create a vector of the variables with the correct prefix.
        varsToSelect <- paste(letters[i], vars, sep = "_")
        # Add pidp to this vector (no prefix for pidp)
        varsToSelect <- c("pidp", varsToSelect)
        # Now read the data. 
        data <- fread(files[i], select = varsToSelect)
        if (i == 1) {
                all7 <- data  
        }
        else {
                all7 <- full_join(all7, data, by = "pidp")
        }
        # Now we can remove data to free up memory
        rm(data)
} 
```

**all7** now contains the data from all seven waves.

```{r}
head(all7, 3)
```

I will now save this file for future use using the **saveRDS** function in the *myData* folder (make sure first you have this folder on your computer). 

```{r}
saveRDS(all7, "myData/all7.rds")
```







<!--chapter:end:iteration.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Tidy data

> For this class please read ch.12 on Tidy Data from R for Data Science -- http://r4ds.had.co.nz/tidy-data.html.


In the previous part of the course (joiningData.Rmd) we learned how to join together data from seven waves of the Understanding Society. Let us open this data set.

```{r cache = TRUE}
UndSoc <- readRDS("myData/all7.rds")
head(UndSoc)
```

Now we will work on how these data can be represented and prepared for the analysis. Please read ch.12 on Tidy Data from the R for Data Science Book -- http://r4ds.had.co.nz/tidy-data.html.

## Long and wide formats

Let us keep only a few observations and columns in the data and more closely look at its structure.

```{r cache = TRUE}
UndSocExample <- UndSoc %>%
  filter(pidp == 68001367 | pidp == 68004087) %>%
  select(pidp, a_sex: b_fimnnet_dv)
UndSocExample
```

These are the data for two individuals only in waves 1 and 2. The data are represented in the wide format. This means that we have one row for each individual, and data from different waves are recorded in several columns. For example, the data on sex from wave 1 is in column *a_sex* and the data on sex from wave is in *b_sex*.

You will find this representation of the data common in longitudinal data sets. It may be convenient for certain purposes, but it is generally recommended to keep the data in the long format (that corresponds to the tidy data principles as described in the R for Data Science book).

To move from the wide to the long format we can use the function **melt** and **cast** functions from the **reshape2** package.

```{r cache = TRUE}
require(reshape2)

# First we "melt" the data frame.
UndSocExampleMolten <- UndSocExample %>%
  melt(id = "pidp")
UndSocExampleMolten

# Next I want to split the column variable into a column indicating wave and a column indicating variable name. 
# I will use the function separate() from tidyr.

UndSocExampleSep <- UndSocExampleMolten %>%
        separate(variable, into = c("wave", "variable"), sep = "_")
UndSocExampleSep

# We have a problem here because one of our variables (fimnnet_dv) has _ in the name and we do not want to separate by it. To avoid this problem we need to add the argument extra = "merge"" in separate().

UndSocExampleSep <- UndSocExampleMolten %>%
        separate(variable, into = c("wave", "variable"), sep = "_", extra = "merge")
UndSocExampleSep

# Next we "cast" the molten data frame into the format we want.

UndSocExampleLong <- UndSocExampleSep %>%
  dcast(pidp + wave ~ variable)
UndSocExampleLong
```

Now the data are in the "long format". This means that we have as many rows for each individual as the number of waves, a variable indicating wave, and all other variables are in columns. In most cases with longitudinal data, the long format is easier to work with.

What if we want to convert the data back to the wide format?

```{r cache = TRUE}
# First melt
UndSocExampleMolten2 <- UndSocExampleLong %>%
  melt(id = c("pidp", "wave"))
UndSocExampleMolten2

# Unite the columns
UndSocExampleUnited <- UndSocExampleMolten2 %>%
  unite("variable", c("wave", "variable"), sep = "_")
UndSocExampleUnited

# And now cast
UndSocExampleWide <- UndSocExampleUnited %>%
  dcast(pidp ~ variable)
UndSocExampleWide
```

We can also restructure the data using the **gather** and **spread** functions from the **tidyr** package (part of **tidyverse**). **gather** is roughy equivalent to **melt** and **spread** is roughy equivalent to **dcast**.

Moving from wide to long:

```{r cache = TRUE}
UndSocExample

# This "melts" the data frame. 
UndSocExample %>%
  gather(a_sex:b_fimnnet_dv, key = "variable", value = "value")

# Next we want to split the "variable" column and "cast" in the long format
UndSocExample %>%
  gather(a_sex:b_vote6, key = "variable", value = "value") %>%
  separate(variable, into = c("wave", "variable"), sep = "_", extra = "merge") %>%
  spread(key = variable, value = value)

```

If we want to move from long to wide:

```{r cache = TRUE}
UndSocExampleLong

UndSocExampleLong %>%
  gather(dvage:vote6, key = "variable", value = "value") %>%
  unite("variable", c("wave", "variable"), sep = "_") %>%
  spread(key = variable, value = value)

```

**Exercise**. Reshape the full **UndSoc** data frame from wide to long format. Call the object where you will store the result **UndSocLong**.

**Solution:**

```{r cache = TRUE}
UndSocLong <- UndSoc %>%
  gather(a_sex:g_fimnnet_dv, key = "variable", value = "value") %>%
  separate(variable, into = c("wave", "variable"), sep = "_", extra = "merge") %>%
  spread(key = variable, value = value)
head(UndSocLong, 5)
```

## Cleaning the data

Before we begin the analysis we want to make sure that the data have been cleaned and all the missing values have been correctly identified. It usually makes sense to separate the cleaning and analysis stages into separate scripts.

Let us explore the data set we have. Note that if we had not converted the data into the long format we would have to tabulate and clean each variable seven times.

```{r cache = TRUE}
summary(UndSocLong)
table(UndSocLong$wave)
table(UndSocLong$dvage)
table(UndSocLong$sex)
table(UndSocLong$vote6)
summary(UndSocLong$fimnnet_dv)
```

Note the negative values for dvage, sex and vote6. These are missing values that need to be coded as missing.

```{r cache = TRUE}
UndSocLong  <- UndSocLong %>%
  mutate(dvage = ifelse(dvage > 0, dvage, NA)) %>%
  mutate(sex = ifelse(sex > 0, sex, NA)) %>%
  mutate(vote6 = ifelse(vote6 > 0, vote6, NA))
table(UndSocLong$dvage)
table(UndSocLong$sex)
table(UndSocLong$vote6)
```

We also have negative values for income (**fimnnet_dv**), but we will leave as it is for now.

We may also want to code **sex** as "male" and "female" and assign meaningful labels to **vote6**.

```{r cache = TRUE}
UndSocLongClean <- UndSocLong %>%
  mutate(sex = recode(sex, "1" = "male", "2" = "female")) %>%
  mutate(vote6 = recode(vote6, "1" = "very", "2" = "fairly", "3" = "not very", "4" = "not al all"))
head(UndSocLongClean, 10)

saveRDS(UndSocLongClean, "myData/all7clean.rds")

```



<!--chapter:end:tidyData.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Data visualisation {#datavis}

> Pre-requisite for this class: ch.3 ("Data visualisation") from R for Data Science - http://r4ds.had.co.nz/data-visualisation.html

At home you learned about the basic principles of data visualisation in R with the **ggplot2** package. Let us see how we can apply this to the Understanding Society data set.

Personally I can never remember all the details of the ggplot2 syntax. I often use the ready-made "recipes" from the R Graphics Cookbook by W.Chang -- https://www.amazon.co.uk/R-Graphics-Cookbook-Winston-Chang/dp/1449316956/. The 2nd edition is coming out later this year -- https://www.amazon.co.uk/Graphics-Cookbook-2e-Winston-Chang/dp/1491978600 .

You may also find Winston Chang's website useful (and not only for graphics) - http://www.cookbook-r.com .

## Reading in the data

First let us read in the data we used in week 2 when we learned about **dplyr** (a short version of the wave 1 data) and recreate the measures for weight, height and BMI.

```{r cache = TRUE}
library(tidyverse)
library(data.table)
W1 <- readRDS("myData/W1mod.rds")
head(W1, 3)
```

## Visualising one quantitative variable

**Exercise**. Visualise the distribution of the BMI with **ggplot2**. Which statistical graphs would be appropriate for this?


### Histogram.

```{r cache = TRUE}
ggplot(W1, aes(x=bmi)) +
  geom_histogram(bins = 100) +
  xlab("Body mass index")
```

### Density chart.

```{r cache = TRUE}
ggplot(W1, aes(x=bmi)) +
  geom_density() +
  xlab("Body mass index")
```


## Visualising one categorical variable

**Exercise**. Visualise the distribution of *a_ukborn* with **ggplot2**. Which statistical graphs would be appropriate for this?

### Bar plot.

```{r cache = TRUE}
table(W1$a_ukborn)
W1 <- W1 %>%
  mutate(a_ukborn = ifelse(a_ukborn > 0, a_ukborn, NA)) %>%
  mutate(cbirth = recode(a_ukborn, "1" = "England", 
                         "2" = "Scotland",
                         "3" = "Wales",
                         "4" = "Northern Ireland",
                         "5" = "Not UK")) 
  
table(W1$cbirth)
W1 %>% 
  filter(!is.na(cbirth)) %>%
  ggplot(aes(x=cbirth)) +
  geom_bar() +
  xlab("Country of birth")

table(W1$cbirth, useNA = "always")
```

## Visualising two quantitative variables

**Exercise**. Visualise the joint distribution of weight (in kg) and height (in cm). In your chart show the regression line and the nonparametric smoothing line.

```{r cache = TRUE}
ggplot(W1, aes(x = weightkg, y= heightcm)) +
  geom_point() +
  geom_smooth() +
  stat_smooth(method=lm)
```

## Visualising one categorical and one quantitative variable

**Exercise**. Visualise the distribution of BMI for a) men and women, b) different age groups.

```{r cache = TRUE}
# Coding a categorical variable for age groups

table(W1$a_dvage, useNA = "always")

W1 <- W1 %>%
        mutate(agegr = ifelse(a_dvage < 31, "16-30",
                              ifelse(a_dvage > 30 & a_dvage < 46, "31-45",
                                ifelse(a_dvage > 45 & a_dvage < 61, "46-60",
                                       ">60")))) %>%
        mutate(agegr = factor(agegr, c("16-30", "31-45", "46-60", ">60")))

ggplot(W1, aes(x = agegr, y= bmi)) +
  geom_boxplot() +
  xlab("Age group") +
  ylab("Body mass index")
```

## Visualising two categorical variables

**Exercise**. Use facets to visualise the distribution of *a_ukborn* by age group.

```{r cache = TRUE}
W1 %>% 
  filter(!is.na(cbirth)) %>%
  ggplot(aes(x=cbirth)) +
  geom_bar() +
  xlab("Country of birth") +
  facet_wrap(~ agegr)
```

Alternatively you can do a jitter plot, but in our case it wouldn't look nice. 

```{r cache = TRUE}
W1 %>% 
  filter(!is.na(cbirth)) %>%
  ggplot(aes(x=cbirth, y = agegr)) +
  geom_jitter() +
  xlab("Country of birth") +
  ylab("Age group")
```

## Showing the relationships by group

**Exercise**. Use facets to visualise the association between age and BMI by country of birth.

```{r cache = TRUE}
W1 %>%
        filter(!is.na(cbirth)) %>%
        ggplot(aes(x = a_dvage, y= bmi)) +
                geom_point() +
                geom_smooth() +
                facet_wrap(~ cbirth)
```


















<!--chapter:end:dataVis.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Data visualisation 2 {#datavis2}

> Pre-requisite for this class: ch.3 ("Data visualisation") from R for Data Science - http://r4ds.had.co.nz/data-visualisation.html

> Also see the ggplot2 cheat sheet - https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf

At the previous class we looked at some simple data visualisation techniques (see http://abessudnov.net/dataanalysis3/datavis.html). Now we will explore how we can visualise longitudinal data.

First we want to read in the data frame we created when we learned to join and tidy data.

```{r}
UndSocLong <- readRDS("myData/all7clean.rds")
```

## Visualising income

Now we want to visualise the distribution of income. There are several ways of doing this.

First, we can create box plots for each wave.

```{r cache = TRUE}
ggplot(UndSocLong, aes(x = wave, y= fimnnet_dv)) +
  geom_boxplot() +
  xlab("Wave") +
  ylab("Net monthly income")
```

The chart does not look nice because of the outliers. We can either remove the outliers or display only a specified range of the box plots. 

```{r cache = TRUE}
ggplot(UndSocLong, aes(x = wave, y= fimnnet_dv)) +
  geom_boxplot() +
  xlab("Wave") +
  ylab("Net monthly income") +
  ylim(-500, 5000)
```

We can see that the median income has been slowly increasing.

Another way to display these data is a density plot. 

```{r cache = TRUE}
ggplot(UndSocLong, aes(x = fimnnet_dv)) +
  geom_density() +
  xlab("Net monthly income") +
  xlim(-500, 5000)
```

We see a peak at zero incomes, and for positive incomes the distribution is close to normal.

We can also create this chart for each wave separately.

```{r cache = TRUE}
ggplot(UndSocLong, aes(x = fimnnet_dv)) +
  geom_density() +
  xlab("Net monthly income") +
  xlim(-500, 5000) +
  facet_wrap(~ wave)
```

Now we may want to visualise the change of income across the waves.

We can plot the income trajectories for each individual in the data set. Let us do this for the first five individuals.

```{r cache = TRUE}
# 5 individuals, 7 waves: 5*7 = 35
first5 <- UndSocLong %>%
        slice(1:35) %>%
        select(pidp, wave, fimnnet_dv)
kable(first5)
ggplot(first5, aes(x = wave, y = fimnnet_dv, colour = as.factor(pidp))) +
        geom_point(na.rm = TRUE) + 
        geom_line(aes(group = pidp), na.rm = TRUE) +
        ylab("Net monthly income") + 
        xlab("Year")
```

This chart nicely illustrates individual income trajectories, but if we try this for even 100 people the chart will be a complete mess.

```{r cache = TRUE}
UndSocLong %>%
        slice(1:700) %>%
        ggplot(aes(x = wave, y = fimnnet_dv, group = pidp)) +
                geom_point(na.rm = TRUE) + 
                geom_line(na.rm = TRUE) +
                ylab("Net monthly income") + 
                xlab("Wave")
```

Instead we can visualise summary statistics, such as mean or median. For a variable with outliers such as income median would be a better summary.

```{r cache = TRUE}
# First we create a data frame of medians.
medians <- UndSocLong %>%
        group_by(wave) %>%
        summarise(
                medianIncome = median(fimnnet_dv, na.rm = TRUE)
        )
kable(medians)

# Then visualize.
# We need to add group = 1 because wave is not numeric
ggplot(medians, aes(x = wave, y = medianIncome, group = 1)) +
        geom_point() + 
        geom_line() + 
        ylab("Median net monthly income") + 
        xlab("Wave")

```

There are a number of things we may want to do with this chart. First, we may want to change wave to year. Second, we may want to display confidence intervals for the point estimates.

First, let us create a variable for year. We need to do this in the original data frame. For each wave in the Understanding Society, the data were collected for two years. So for Wave 1 the field work was conducted in 2009 and 2010. For simplicity, I will code the first year only.

```{r cache = TRUE}
UndSocLong <- UndSocLong %>%
        mutate(year = recode(wave, "a" = "2009",
                             "b" = "2010",
                             "c" = "2011",
                             "d" = "2012",
                             "e" = "2013",
                             "f" = "2014",
                             "g" = "2015")) %>%
        mutate(year = as.numeric(year))

```

Producing confidence intervals for the median is not straightforward (you need to use statistical simulation or some other statistical tricks) so I'll use the mean instead.

```{r cache = TRUE}
UndSocLong %>%
        group_by(year) %>%
        summarise(
                # I use the function t.test to get the means and standard errors
                meanIncome = t.test(fimnnet_dv)$estimate,
                # Here I calculate the 95% confidence interval
                lowerIncome = t.test(fimnnet_dv)$conf.int[1],
                upperIncome = t.test(fimnnet_dv)$conf.int[2]
        ) %>%
        # and now I visualise
        ggplot(aes(x = year, y = meanIncome)) +
                geom_point() + 
                geom_line() +
                geom_ribbon(aes(ymin=lowerIncome, ymax=upperIncome), alpha=0.2) + 
                ylab("Median net monthly income") + 
                xlab("Year")

```

The confidence intervals are quite wide. This is not surprising given our sample size.

Another chart we can produce is not only for the medians (or means), but for different quantiles. Let us take the following quantiles: 0.01, 0.05, 0.1, 0.5. 0.9, 0.95, 0.99. This will show the change in income for the 1% poorest, 5% poorest, 10% poorest, etc. 

This can be done in several ways. If we use summarise to produce multiple quantiles it can get clumsy.


```{r cache = TRUE}
UndSocLong %>%
        group_by(year) %>%
        summarise(
                quant1 = quantile(fimnnet_dv, 0.01, na.rm = TRUE),
                quant5 = quantile(fimnnet_dv, 0.05, na.rm = TRUE),
                quant10 = quantile(fimnnet_dv, 0.1, na.rm = TRUE),
                quant50 = quantile(fimnnet_dv, 0.5, na.rm = TRUE),
                quant90 = quantile(fimnnet_dv, 0.9, na.rm = TRUE),
                quant95 = quantile(fimnnet_dv, 0.95, na.rm = TRUE),
                quant99 = quantile(fimnnet_dv, 0.99, na.rm = TRUE)
        ) %>%
        ungroup() %>%
        # Now I need to convert this to a long format
        gather(quantile, value, quant1:quant99) %>%
        # and plot
        ggplot(aes(x = year, y = value, colour = quantile)) +
                geom_point(na.rm = TRUE) + 
                geom_line(na.rm = TRUE) +
                ylab("Net monthly income") + 
                xlab("Year")

```

A more economical way to write the code is the following. This time I will also select only people with positive net incomes.


```{r cache = TRUE}
library(broom)
UndSocLong %>%
        filter(fimnnet_dv > 0) %>%
        nest(-year) %>%
        mutate(Quantiles = map(data, ~ quantile(.$fimnnet_dv,
                                       c(0.01, 0.05, 0.1, 0.5, 0.9, 0.95, 0.99)))) %>%
        unnest(map(Quantiles, tidy)) %>%
        ggplot(aes(x = year, y = x, colour = names)) +
                geom_point(na.rm = TRUE) + 
                geom_line(na.rm = TRUE) +
                ylab("Net monthly income") + 
                xlab("Year")

```



<!--chapter:end:datavis2.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# R Markdown {#rmarkdown}

> For this class read ch.27 ("R Markdown") from R for Data Science - http://r4ds.had.co.nz/r-markdown.html

The traditional approach for statistical data analysis is to analyse the data and write up the results separately. For example, you may conduct your analysis in R and then copy the output to Word and write up your results there. This is an error prone approach. The code and the results are not synchronised so that if you change your code the results will not change automatically. You can also make mistakes when copying the results.

R Markdown was designed to combine statistical analysis and communication into a single framework. The code and the results are combined in a single document, and you can also add text, external tables and images if you want. You can use R Markdown to produce documents in different formats (html, Word, pdf, presentation slides).

This website has been produced using R Markdown.

There are many places on the web where you can learn the basics of R Markdown and there is no point for me to repeat this here. Please see ch.27 from R for Data Science (http://r4ds.had.co.nz/r-markdown.html) and the official R Markdown website (https://rmarkdown.rstudio.com) and follow the links. You can also check this webpage: https://stat545.com/block007_first-use-rmarkdown.html (this is part of a course at the University of British Columbia that is similar to our module, but somewhat more advanced).

Here I will focus on a few things that are specifically relevant for your reports.

1. You can knit your reports in either pdf or Word formats. If you are going to use Word you will not be able to use the **stargazer** package for regression output. You may try to use a combination of the packages **memisc** and **pander** to achieve the same result (see https://stackoverflow.com/questions/24342162/regression-tables-in-markdown-format-for-flexible-use-in-r-markdown-v2), but I haven't tried this and I cannot guarantee that it will work. Also see https://rmarkdown.rstudio.com/articles_docx.html

If you want to knit as pdf you will need to install LaTeX first (https://www.latex-project.org). Install LaTeX (complete version), restart your computer, restart R Studio and it should all work automatically.

2. If you knit as pdf with LaTeX **stargazer** will work just fine, but you need to set the *results* argument to 'asis' and run  **stargazer** in a separate R chunk. The code will look something like

<pre><code>```{r results = 'asis'}
stargazer(m1, m2, m3, m4, type = "latex")
```</pre></code>

3. You need to include all your code in your report. You do not need to include messages and warnings. You may also want to use cache to speed up rendering of your document. To achieve this result include in the beginning of your R Mardown file an R chunk setting the following global options.

<pre><code>```{r setup}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)
```</pre></code>

If you experience problems with caching switch it to FALSE.

4. To knit a document you can use a button in R Studio. In my experience, sometimes it does not work as intended. Then you can use the command line:

```{r eval = FALSE}
rmarkdown::render("your_file.Rmd", "rmarkdown::word_document")
rmarkdown::render("your_file.Rmd", "rmarkdown::pdf_document")
```

5. You will want to include a bibliography in your report. An easy way to do this is simply to type it in the end of your document. However, this is not the most efficient way of doing this. I recommend you use Zotero for your bibliographies. You will also be able to use Zotero for essays for other modules and for your dissertation, even if you do not use R or R Markdown and write everything in Word. Once you master Zotero referencing and creating bibliographies will become much easier.

a) Go to the Zotero website (https://www.zotero.org) and create an acount.

b) Download and install the Zotero client for your computer (https://www.zotero.org/download/).

c) Download and install a Zotero plugin for your web browser. Once you have done this you will be able to automatically save references to your Zotero libraries from Google Scholar and other sources.

d) You can use Zotero with Word to reference and automatically create bibliographies. This is not relevant for this module, but for other modules and for your dissertation this is a very useful skill. To learn how to do this see https://www.zotero.org/support/word_processor_integration

e) To use Zotero with R Markdown you need to do the following. Once you have added all the references to your library, export it as a bib file (File -> Export Library -> choose BibTex as the format). Save your bib file in the same folder as your R Markdown document.

f) To learn how to reference and cite in R Markdown see https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html

g) To include a bibliography add the following line to the YAML header of your R Markdown document:

bibliography: your_bibfile_name.bib

h) The file *example.Rmd* available in the Github repo shows the minimal example of referencing and creating a bibliography with one source (see https://raw.githubusercontent.com/abessudnov/dataanalysis3/master/example.Rmd). The associated bib file is available here: https://raw.githubusercontent.com/abessudnov/dataanalysis3/master/example.bib.

i) The bib file can be opened and edited in any text editor and you can also use special software to do this, such as JabRef (http://www.jabref.org).















<!--chapter:end:rmarkdown.Rmd-->

```{r include=FALSE, cache=FALSE}
# This is a script to remove all the objects from R memory to start every Rmd file from fresh

rm(list = ls())

# I also set knitr options here

knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)

```
# Modelling {#modelling}

> Pre-requisite for this class: ch.22-24 ("Model") from R for Data Science - http://r4ds.had.co.nz/model-intro.html

Let us explore how we can apply linear models with the Understanding Society data for your reports.

First we need to read in the data. This is the same data frame that I used for Data visualisation 2. I saved it then so now I can simply open the file.

```{r message = FALSE}
library(tidyverse)
UndSoc <- readRDS("myData/all7clean.rds")
```

We have a variable for interest in politics (**vote6**) and we will take it as the outcome variable we want to study. It is a discrete variable measured on a four-point scale (from "not at all interested" to "very interested"). It is an ordinal rather than continuous variable so strictly speaking it is not statisticaly appropriate to calculate the mean of this variable and to use simple linear regression. You will see though that we can still do this and learn something interesting from this exercise.

First let us look at the distribution of the variable.

```{r}
table(UndSoc$vote6)
```

To do the modelling we want to recode it to numeric. I will do this in such a way that larger values indicate stronger interest in politics.

```{r}
UndSoc <- UndSoc %>%
  mutate(polinterest = recode(vote6, 
                              "very" = "4", "fairly" = "3",
                              "not very" = "2", "not al all" = "1")) %>%
  mutate(polinterest = as.numeric(polinterest))

table(UndSoc$polinterest)
```

## Cross-sectional analysis

Let us start from cross-sectional analysis, i.e. the analysis of the data at one point in time. You can choose any wave to do this; I will go with wave 1.

First I create a separate data frame for wave 1 only.

```{r}
wave1 <- UndSoc %>%
  filter(wave == "a") %>%
  filter(!is.na(dvage))
```

### Age and political interest

We will start with the association between age and political interest. It is always a good idea to start with visualisations.

```{r}
wave1 %>%
  ggplot(aes(x = dvage, y = polinterest)) +
  geom_smooth()
```

I fit a non-parametric smooth here with **geom_smooth**, and the association between age and political interest seems to be non-linear. 

We can also fit a regression line and see how well it describes the data.

```{r}
wave1 %>%
  ggplot(aes(x = dvage, y = polinterest)) +
  geom_smooth() +
  geom_smooth(method = "lm", colour = "red")
```

Up to the age of about 70 years two lines are pretty close and I'd say that the linear function adequately describes the data. However, for people older than 75 the linear assocition is inadequate and provides a really poor fit.

### Sex and political interest

Let us look at a bar chart showing the association between sex and political interest.

```{r}
wave1 %>%
  ggplot(aes(x = sex, y = polinterest)) +
  geom_bar(stat = "summary", fun.y = "mean")
```

We see that mean political interest is somewhat higher for men compared to women.

### Linear model

As I said above, the outcome variable is ordinal so fitting a linear model will have some limitations. For our purposes this is fine though.

For your reports you should fit a linear model for ordered and binary outcomes. The linear model for binary outcomes is called the linear probability model. Please do not estimate logit or probit models for the purpose of this assignment. Of course, you cannot fit a linear model for nominal outcomes. If you outcome is nominal (this is unlikely), you should either stay with descriptive statistics and graphs or use a multinomial logit model. You should only do this if you are confident that you know what you are doing. 

Now we can fit a simple linear regression model with two predictors: age and sex.

```{r}
m1 <- lm(polinterest ~ sex + dvage, wave1)
summary(m1)
```

Both age and sex are highly statistically significant predictors of political interest. Men on average are 0.3 points higher on the political interest scale. For age, a one-year difference is associated with about 0.01 change in political interest (older people are more interested). For two people with the age difference of about 30 years this corresponds to the difference in political interest of about 0.3. 

We have seen that the association between age and political interest is non-linear. To model this, we may want to include the quadratic term for age.

```{r}
m2 <- lm(polinterest ~ sex + dvage + I(dvage^2), wave1)
summary(m2)
```

The quadratic term is statistically significanty indicating a non-linear fit. Note, however, that the coefficients are now more difficult to interpret. We can visualise the association between age and political interest to get some idea of the effect size.

```{r}
wave1 %>%
  ggplot(aes(x = dvage, y = polinterest)) +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2))
```

Note that this visualisation does not control for sex.

### Interaction effect between sex and age

Another question we can ask is whether the association between age and political interest is the same or different for men and women. Note that in the previous model we control for sex, and the coefficient for age represents an averaged association between age and political interest for men and women. It is possible to have a situation where the association is very different for two sexes (for example, for men political interest increases with age and for women it decreases). To check this formally, we can fit a model with an interaction effect. 

```{r}
m3 <- lm(polinterest ~ sex * dvage, wave1)
summary(m3)
```

We see that the interaction effect is indeed statistically significant and negative suggesting that for men the association between age and political interest is weaker.

We can also fit a quadratic model.

```{r}
m4 <- lm(polinterest ~ sex * dvage + sex * I(dvage^2), wave1)
summary(m4)
```

The interaction effect is highly statistically significant. To make sense of the association between age and political interest for men and women it is best to visualise this model.

```{r}
wave1 %>%
  ggplot(aes(x = dvage, y = polinterest, colour = sex)) +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2))
```

We can see that the lines for men and women are indeed different, although the non-linear pattern of the association is similar for both sexes.

### Checking the assumptions

In Data Analysis 2 your task was to check the assumptions underlying linear regression models. This is an important step, but it is less relevant for our work in this module. Many of these assumptions are about the distribution of residuals and affect standard errors of regression coefficients. This is very important for small samples, but less important for large samples like the one we have in the Understanding Society where standard errors are usually small compared to regression coefficients and moderate violation of the regression assumpions (such as not normal distribution of the residuals and heteroskedasticity) will not affect the results much. You do not have to show how you have checked the assumptions in your reports (although if you want to do it for yourself this would not hurt).

Another common question is the interpretation of the R-squared coefficient. In Model 4 the R-squared coefficients was about 0.05 suggesting that age and sex can jointly account for about 5% of the variance of political interest. Is this an indication that the model is really poor? Yes, if your task is to build a model that predicts political interest well. This shows that just knowning a person's age and sex you can only make a wild guess about their level of political interest (this isn't really surprising).

However, if our task is simply to explore the association between age, sex and political interest rather than to build a model that predicts political interest well, then we should not pay too much attention to R-squared.

### Presenting the output for multiple regression models

How to present the regression output in your reports? You can of course simply fo **summary(m1)** as we did above, but this does not look really nice. Another way is to use the package **stargazer**. Note that **stargazer** will only work if you knit your reports as pdf rather than Word. To knit as pdf you  will need to install LaTeX on your computers (see https://www.latex-project.org). This is how the table produced by **stargazer** will look like.

```{r results = "asis"}
library(stargazer)
stargazer(m1, m2, m3, m4, type = "html")
```

In your reports you will knit as LaTeX rather than html so you should do something like:

<pre><code>```{r results = 'asis'}
stargazer(m1, m2, m3, m4, type = "latex")
```</pre></code>

The *results* argument shoud be set to *asis* so that the results are displayed correctly. Note that *type* is *latex*.

**stargazer** has many options to customise the tables. Please do experiment with them.

## Longitudinal analysis

Now we may want to model how things change over time (i.e. do longitudinal modelling). 

### Simple model

We will start from simply plotting mean political interest over time.

```{r}
# First let us code the variable for year.
UndSoc <- UndSoc %>%
  mutate(year = dplyr::recode(wave, "a" = "2009",
                       "b" = "2010",
                       "c" = "2011",
                       "d" = "2012",
                       "e" = "2013",
                       "f" = "2014",
                       "g" = "2015")) %>%
  mutate(year = as.numeric(year))

UndSoc %>%
    group_by(year) %>%
    summarise(
      meanPI = mean(polinterest, na.rm = TRUE)
    ) %>%
    ggplot(aes(x = year, y = meanPI)) +
    geom_point() +
    geom_line()

```

The model that describes this chart will be the following.

```{r}
m5 <- lm(polinterest ~ as.factor(year), UndSoc)
summary(m5)
```

We can see from this model that in 2011 to 2013 political interest was statistically significantly lower than in 2009, and in 2015 it was statistically significantly higher.

### Adding a time-constant variable

We may want to check if the change in political interest depends on a time-constant variable such as sex. Was the change in mean political interest similar for men and women?

```{r}
UndSoc %>%
    filter(!is.na(sex)) %>%
    group_by(year, sex) %>%
    summarise(
      meanPI = mean(polinterest, na.rm = TRUE)
    ) %>%
    ggplot(aes(x = year, y = meanPI, colour = sex)) +
    geom_point() +
    geom_line()
```

It does not look like there was an interaction between sex and year, but we can check it formally.

```{r}
m6 <- lm(polinterest ~ as.factor(year)*sex, UndSoc)
summary(m6)
```

This suggest that the gap in political interest between men and women was slighty hgher in 2013, but the effect size is really small.

### Adding a time-varying variable

Things get more tricky (and more interesting) if we want to add a time-varying variable such as age to the analysis. We have already modelled the association between age and political interest cross-sectionally. This answers the question of whether there is any difference in political interest between people of different age.

Another question that we may want to ask is whether political interest changes when people get older. Note that this is a different question, and answering it requires the use of longitudinal data.

To answer this question we need to apply regression models with fixed effects. You have not covered these models in Data Analysis 2. The main idea is to look at the changes for the same individuals. We want to see if for each person in the data getting older is associated with the changes in political interest, and then we can average the effects across different people. In other words, instead of fitting the model that compares *between* individuals we want to compare *within* individuals.

Technically, we can achieve this by simply controlling for individual id.

With the data of our size the model will be very difficult to estimate, so for the demonstration purposes I will select 500 random individuals from the data and run the model for them.

```{r}
# creating a data frame with data for 500 random people
set.seed(1)
random500 <- sample(unique(UndSoc$pidp), 500)
UndSoc500 <- UndSoc %>%
  filter(pidp %in% random500)

m7 <- lm(polinterest ~ as.factor(year) + as.factor(pidp) + dvage, UndSoc500)
```

If we run **summary(m7)** now we will get a really long output with 499 coefficients for **pidp**. I will use **stargazer** to present the results and will omit these coefficients (called individual fixed effects) and the coefficients for year.

```{r results = 'asis'}
stargazer(m7, omit = c("pidp", "year"), type = "html")
```

Note that the coefficient for age is not statisticaly significant. This indicates that as people get older their interest in politics does not change much, at least in our sub-sample of 500 people.

A more efficient way to estimate fixed effects model is to use the package **plm**.

```{r}
library(plm)
m8 <- plm(polinterest ~ dvage, data = UndSoc500, model = "within", index = c("pidp", "year"),
          effect = "twoways")
summary(m8)
```

Asking for the **twoways** effect means that we fit the model with individual and year fixed effects.

Note that the effect size is exactly the same in models 7 and 8 (becuase this is essentially the same model), but **plm** will work faster. With **plm** you may be able to estimate the fixed effects model with the full data for your reports, although the estimation will take you some time. I will not do this here.

## Further reading

If you want to use linear models in your assignment I strongly recommend you consult the following book (available as an e-book in the library):

1. J.Fox, S.Weisberg. (2011). An R Companion to Applied Regression. 2nd ed. Sage.

The chapter on factors and interactions will be particularly useful.

If you want to know more about fixed effects models see

2. P.D.Allison. (2008). Fixed Effect Regression Models. Sage.

To learn how to use the package **plm** read

3. Y.Croissant, G.Millo. Panel Data Econometrics in R: The plm Package. https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf 

<!--chapter:end:modelling.Rmd-->

